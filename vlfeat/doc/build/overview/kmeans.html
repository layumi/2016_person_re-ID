<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
   <html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <!-- IE Standards Mode -->
  <meta content="IE=edge" http-equiv="X-UA-Compatible"></meta>

  <!-- Favicon -->
  <link href="../images/vl_blue.ico" type="image/x-icon" rel="icon"></link>
  <link href="../images/vl_blue.ico" type="image/x-icon" rel="shortcut icon"></link>

  <!-- Page title -->
  <title>VLFeat - Tutorials > K-means clustering</title>

  <!-- Stylesheets -->
  <link href="../vlfeat.css" type="text/css" rel="stylesheet"></link>
  <link href="../pygmentize.css" type="text/css" rel="stylesheet"></link>
  <style xml:space="preserve">
    /* fixes a conflict between Pygmentize and MathJax */
    .MathJax .mo, .MathJax .mi {color: inherit ! important}
  </style>
  

  <!-- Scripts-->
  

  <!-- MathJax -->
  <script xml:space="preserve" type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      processEscapes: true,
    },
    TeX: {
      Macros: {
        balpha: '\\boldsymbol{\\alpha}',
        bc: '\\mathbf{c}',
        be: '\\mathbf{e}',
        bg: '\\mathbf{g}',
        bq: '\\mathbf{q}',
        bu: '\\mathbf{u}',
        bv: '\\mathbf{v}',
        bw: '\\mathbf{w}',
        bx: '\\mathbf{x}',
        by: '\\mathbf{y}',
        bz: '\\mathbf{z}',
        bsigma: '\\mathbf{\\sigma}',
        sign: '\\operatorname{sign}',
        diag: '\\operatorname{diag}',
        real: '\\mathbb{R}',
      },
      equationNumbers: { autoNumber: 'AMS' }
      }
    });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" xml:space="preserve" type="text/javascript"></script>

  <!-- Google Custom Search -->
  <script xml:space="preserve">
    (function() {
    var cx = '003215582122030917471:oq23albfeam';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https' ? 'https:' : 'http:') +
    '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
  </script>

  <!-- Google Analytics -->
  <script xml:space="preserve" type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-4936091-2']);
    _gaq.push(['_trackPageview']);
    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
 </head>

 <!-- Body Start -->
 <body>
  <div id="header-section">
    <div id="header">
      <!-- Google CSE Search Box -->
      <div class="searchbox">
        <gcse:searchbox-only autoCompleteMaxCompletions="5" autoCompleteMatchType="any" resultsUrl="http://www.vlfeat.org/search.html"></gcse:searchbox-only>
      </div>
      <h1 id="id-16"><a shape="rect" href="../index.html" class="plain"><span id="vlfeat">VLFeat</span><span id="dotorg">.org</span></a></h1>
    </div>
    <div id="sidebar"> <!-- Navigation Start -->
      <ul>
<li><a href="../index.html">Home</a>
<ul>
<li><a href="../about.html">About</a>
</li>
<li><a href="../license.html">License</a>
</li>
</ul></li>
<li><a href="../download.html">Download</a>
<ul>
<li><a href="../install-matlab.html">Using from MATLAB</a>
</li>
<li><a href="../install-octave.html">Using from Octave</a>
</li>
<li><a href="../install-shell.html">Using from the command line</a>
</li>
<li><a href="../install-c.html">Using from C</a>
<ul>
<li><a href="../xcode.html">Xcode</a>
</li>
<li><a href="../vsexpress.html">Visual C++</a>
</li>
<li><a href="../gcc.html">g++</a>
</li>
</ul></li>
<li><a href="../compiling.html">Compiling</a>
<ul>
<li><a href="../compiling-unix.html">Compiling on UNIX-like platforms</a>
</li>
<li><a href="../compiling-windows.html">Compiling on Windows</a>
</li>
</ul></li>
</ul></li>
<li class='active'><a href="tut.html">Tutorials</a>
<ul>
<li><a href="frame.html">Local feature frames</a>
</li>
<li><a href="covdet.html">Covariant feature detectors</a>
</li>
<li><a href="hog.html">HOG features</a>
</li>
<li><a href="sift.html">SIFT detector and descriptor</a>
</li>
<li><a href="dsift.html">Dense SIFT</a>
</li>
<li><a href="liop.html">LIOP local descriptor</a>
</li>
<li><a href="mser.html">MSER feature detector</a>
</li>
<li><a href="imdisttf.html">Distance transform</a>
</li>
<li><a href="encodings.html">Fisher Vector and VLAD</a>
</li>
<li><a href="gmm.html">Gaussian Mixture Models</a>
</li>
<li class='active' class='activeLeaf'><a href="kmeans.html">K-means clustering</a>
</li>
<li><a href="aib.html">Agglomerative Infromation Bottleneck</a>
</li>
<li><a href="quickshift.html">Quick shift superpixels</a>
</li>
<li><a href="slic.html">SLIC superpixels</a>
</li>
<li><a href="svm.html#tut.svm">Support Vector Machines (SVMs)</a>
</li>
<li><a href="kdtree.html">KD-trees and forests</a>
</li>
<li><a href="plots-rank.html">Plotting AP and ROC curves</a>
</li>
<li><a href="utils.html">Miscellaneous utilities</a>
</li>
<li><a href="ikm.html">Integer K-means</a>
</li>
<li><a href="hikm.html">Hierarchical integer k-means</a>
</li>
</ul></li>
<li><a href="../applications/apps.html">Applications</a>
</li>
<li><a href="../doc.html">Documentation</a>
<ul>
<li><a href="../matlab/matlab.html">MATLAB API</a>
</li>
<li><a href="../api/index.html">C API</a>
</li>
<li><a href="../man/man.html">Man pages</a>
</li>
</ul></li>
</ul>

    </div> <!-- sidebar -->
  </div>
  <div id="headbanner-section">
    <div id="headbanner">
      <span class='page'><a href="tut.html">Tutorials</a></span><span class='separator'>></span><span class='page'><a href="kmeans.html">K-means clustering</a></span>
    </div>
  </div>
  <div id="content-section">
    <div id="content-wrapper">
      <div id="content">
        
    

<div class='toc'>
<h3>Table of Contents</h3><ul><li class="level1"><a href="#tut.kmeans.introduction">Running K-means</a></li>
<li class="level1"><a href="#tut.kmeans.initialization">Choosing an initialization method</a></li>
<li class="level1"><a href="#tut.kmeans.algorithm">Choosing an optimization algorithm</a></li>
</ul>
</div><!-- Table of contents -->


<p>This tutorial shows how to use the <a shape="rect" href="">K-means
algorithm</a> using the VlFeat implementation of Llloyd's algorithm as
well as other faster variants.</p>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.kmeans.introduction">Running K-means</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>KMeans is a clustering algorithm. Its purpose is to partition a set
of vectors into $K$ groups that cluster around common mean
vector. This can also be thought as <em>approximating</em> the input
each of the input vector with one of the means, so the clustering
process finds, in principle, the best dictionary or codebook to
<em>vector quantize</em> the data.</p>

<p>Consider a dataset containing 1000 randomly sampled 2D points:</p>

<div class="highlight"><pre><span class="n">numData</span> <span class="p">=</span> <span class="mi">5000</span> <span class="p">;</span>
<span class="n">dimension</span> <span class="p">=</span> <span class="mi">2</span> <span class="p">;</span>
<span class="n">data</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span><span class="n">numData</span><span class="p">)</span> <span class="p">;</span>
</pre></div>


<p>The function <code/>vl_kmeans</code> can be used to cluster the data
into, say, 30 groups:</p>

<div class="highlight"><pre><span class="n">numClusters</span> <span class="p">=</span> <span class="mi">30</span> <span class="p">;</span>
<span class="p">[</span><span class="n">centers</span><span class="p">,</span> <span class="n">assignments</span><span class="p">]</span> <span class="p">=</span> <span class="n">vl_kmeans</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numClusters</span><span class="p">);</span>
</pre></div>


<p>By default, this uses the
the <a shape="rect" href="">Lloyd</a> algorithm, a method that
alternates between optimizing the cluster centers and the
data-to-center assignments. Once this process terminates, the
matrix <code/>centers</code> contains the cluster centers and the
vector <code/>assignments</code> the (hard) assignments of the input
data to the clusters. The cluster centers are also
called <em>means</em> because it can be shown that, when the
clustering is optimal, the centers are the means of the corresponding
data points. The cluster centers and assignments can be visualized as
follows:</p>

<div class="figure">
  <image src="../demo/kmeans_2d_rand.jpg"></image>
  <div class="caption">KMeans clustering of 5000 randomly sampled data
  points. The black dots are the cluster centers.</div>
</div>

<p>Given a new data point <code/>x</code>, this can be mapped to one of
the clusters by looking for the closest center:</p>

<div class="highlight"><pre><span class="n">x</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">;</span>
<span class="p">[</span><span class="o">~</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="p">=</span> <span class="n">min</span><span class="p">(</span><span class="n">vl_alldist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">centers</span><span class="p">))</span> <span class="p">;</span>
</pre></div>


<p>For larger datastes, this process may be significantly accelerated
by using <a shape="rect" href="kdtree.html">KDTrees</a> or other
approximate nearest neighbor procedures.</p>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.kmeans.initialization">Choosing an initialization method</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>K-means uses local optimization algorithms and is therefore
sensitive to initalization. By default, <code/>vl_kmeans</code>
initializes the cluster centers by picks $K$ data points at
random. Other initalization strategies can be selected as well.
<a shape="rect" href="%dox:kmeans-plus-plus">kmeans++</a> is a popular method that
greedily pick $K$ data points that are maximally different, and can be
use as follows:</p>

<div class="highlight"><pre><span class="p">[</span><span class="n">centers</span><span class="p">,</span> <span class="n">assignments</span><span class="p">]</span> <span class="p">=</span> <span class="n">vl_kmeans</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numClusters</span><span class="p">,</span> <span class="s">&#39;Initialization&#39;</span><span class="p">,</span> <span class="s">&#39;plusplus&#39;</span><span class="p">)</span> <span class="p">;</span>
</pre></div>


<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.kmeans.algorithm">Choosing an optimization algorithm</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>In addition to the original KMeans algorithm proposed by Lloyd,
<code/>vl_kmeans</code> supports two additional
algorithms: <a shape="rect" href="%dox:kmeans-elkan">Elkan's</a> variant, an exact
algorithm using an acceleration technique based the triangular
inequality, and
<a shape="rect" href="">ANN</a>, an approximated algorithm using
approximate nearest neighbours.</p>

<p>These optimization methods can be enabled by setting the
<code/>'Algorithm'</code> parameter to <code/>'Lloyd'</code>,
<code/>'Elkan'</code> or <code/>'ANN'</code> respectively. When using
the <code/>'ANN'</code> algorithm, the user can also specify the
parameters <code/>'MaxNumComparisons'</code>
and <code/>'NumTrees'</code> to <a shape="rect" href="">configure
the KD-tree</a> used used as ANN. In
particular, <code/>'MaxNumComparisons'</code> controls the trade-off
between approximation quality and speed.</p>

<p><code/>vl_demo_kmeans_ann_speed</code> compares the speed of the
three algorithms. Because of the random initialization, each of the
KMeans calls converges to a different local minimum in a different
amount of iterations. In order to measure more accurately the speed of
each pass, a relatively small number
of <code/>'MaxNumIterations'</code> option) is selected. Note that the
speedup factor are in general much more dramatic when truly large
datasets are considered.</p>

<div class="figure">
<image src="../demo/kmeans_speed.jpg"></image>
<div class="caption">Comparisons of Elkan, Lloyd and ANN for different
values of <code/>MaxNumComparison</code>, expressed as a fraction of
the number of clusters. The figure reports the duration, final energy
value, and speedup factor, using both the serial and parallel versions
of the code. The figure was generated using
<code/>vl_demo_kmeans_ann_speed</code>.</div>
</div>


  
      </div>
      <div class="clear">&nbsp;</div>
    </div>
  </div> <!-- content-section -->
  <div id="footer-section">
    <div id="footer">
      &copy; 2007-13 The authors of VLFeat
    </div> <!-- footer -->
  </div> <!-- footer section -->
 </body>
 <!-- Body ends -->
</html>
 